# -*- coding: utf-8 -*-
"""exploratory_analysis(script).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13MKIc30Jd4ya-cTljQ1kkfhP0ZW7q8sj
"""

import pandas as pd
import numpy as np
import math
import seaborn as sns
from collections import Counter
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from google.colab import drive
import ast
import matplotlib.pyplot as plt
from pandas.plotting import table
import re

#reading data from the drive
drive.mount('/content/gdrive',force_remount=True)
df=pd.read_csv('gdrive/My Drive/cp_data.csv',encoding='utf8')

#deleting the first column
del df['Unnamed: 0']
df['Hazard Type'] = df['Hazard Type'].replace({'food additives and flavourings':'food additives and \n flavourings'})
df.fillna('', inplace=True)

#when description is nan, we keep the remote title as a feature
df['Description'] = np.where(df['Description'] == "", df['Remote Title'], df['Description'])

del df['Remote Title']

df.head()

"""
Frequency plot for the hazard types.

"""

sns.set(rc={'figure.figsize':(20,10)})
sns.set(style='whitegrid')
ax = sns.countplot(x="Hazard Type", data=df,color='seagreen', order = df['Hazard Type'].value_counts().index)
ax.set(xlabel='', ylabel='Incidents')
ax.set_title('Frequency of Hazard Types')
total = float(len(df))
for p in ax.patches:
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 3,
            '{0:.1%}'.format(height/total),
            ha="center")

"""
Details for the vocabulary.

"""

#the length for each indcident
x=[len(t) for t in df['Description']]

#the maximum length
max_inc = max(x)

#the minimum length
min_inc = min(x)

#the average length
avg_inc = int(np.mean(x))

#the standard deviation of the length
std_inc = int(np.std(x))

print(f'The maximum length of all the incidences is {max_inc}, while the minimum is {min_inc}.')
print(f'Also the average length for all the incidents is {avg_inc}.')

#violinplot for the distribution of the length of incidents
sns.set(rc={'figure.figsize':(8,6)})
sns.set(style="whitegrid")
ax = sns.violinplot(x=[len(t) for t in df['Description'] if len(t)<round(avg_inc)+std_inc]).set_title('Distribution of the length of incidents')

#we will do the same for each hazard type
hazards = list(Counter(df['Hazard Type']).keys())

def plot_per_category(hazard_type):

  df_filtered = df[df['Hazard Type'] == hazard_type]

  x=[len(t) for t in df_filtered['Description']]

  max_inc = max(x)
  min_inc = min(x)
  avg_inc = int(np.mean(x))
  std_inc = int(np.std(x))

   

  sns.set(rc={'figure.figsize':(6,4)})
  sns.set(style="whitegrid")
  ax = sns.violinplot(x=[len(t) for t in df_filtered['Description'] if len(t)<=round(avg_inc)+std_inc]).set_title(f'Distribution of the length for {hazard_type} incidents')

  return #print(f'The average length for the type:{hazard_type} is {avg_inc}.')

plt.figure(figsize=(20,10))
for hazard in hazards:
    plt.subplot(3,3,hazards.index(hazard)+1)
    plt.subplots_adjust(left=0.125, bottom=4, right=1, top=5,wspace=1, hspace=0.4)
                        
    plot_per_category(hazard)

"""
Tokenization of the text.

"""

#remove the punctuation
tokenizer = RegexpTokenizer(r'\w+')
df["Tokenized"] = [tokenizer.tokenize(x) for x in df['Description']]

#lowercase all the words
df['Tokenized'] = df['Tokenized'].apply(lambda x: [word.lower() for word in x])

#remove all the numbers
df['Tokenized'] = df['Tokenized'].apply(lambda x: [re.sub(r'\d+', '', word) for word in x])

#remove stopwords
df['Tokenized'] = df['Tokenized'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])

#we save the dataframe with the tokenized text
drive.mount('drive')
df.to_csv('tokenizedf_data.csv')
!cp tokenizedf_data.csv "drive/My Drive/"

drive.mount('/content/gdrive',force_remount=True) 
token1_df=pd.read_csv('gdrive/My Drive/tokenizedf_data.csv',encoding='utf8')

del token1_df['Unnamed: 0']

token1_df['Tokenized'] = token1_df['Tokenized'].apply(lambda x: ast.literal_eval(x))

#remove empty strings
token1_df['Tokenized'] = token1_df['Tokenized'].apply(lambda x: [word for word in x if word])

#find the vocabulary size
results = Counter()
token1_df['Tokenized'].apply(results.update)
vocab_size = len(results)
vocab_size

#the 50 most common words of the vocabulary
most_common = results.most_common(50)

#the dataframe with the common words of the vocabulary
com_df = pd.DataFrame(most_common, columns=['Common words', 'Frequency'])

#common words for each hazard type
def common_per_category(hazard_type):

  token_filtered = token1_df[token1_df['Hazard Type'] == hazard_type]

  results = Counter()
  token_filtered['Tokenized'].apply(results.update)

  com_df = pd.DataFrame(results.most_common(20), columns=['Common words', 'Frequency'])


  return com_df

#ploting df with the most common words per hazard type
def plot_df(df):

  fig,ax = plt.subplots(figsize=(6, 10))

  #no axes
  ax.xaxis.set_visible(False)  
  ax.yaxis.set_visible(False)

  #no frame
  ax.set_frame_on(False) 

  #plot table
  tab = table(ax,df ,loc='center',colWidths=[0.3,0.2],edges='open')  

  #set font manually
  tab.auto_set_font_size(False)
  tab.set_fontsize(10) 

  #save the result
  plt.savefig('table.png')

#creating a datframe with the frequency of each word of the vocabulary for all the categories
results = Counter()
token1_df['Tokenized'].apply(results.update)
all = results.most_common(len(results))

all_df = pd.DataFrame(all, columns=['All words', 'Frequency'])


def freq_df(hazard):

  token_filtered = token1_df[token1_df['Hazard Type'] == hazard]

  results = Counter()
  token_filtered['Tokenized'].apply(results.update)
  fr_df = pd.DataFrame(results.most_common(len(results)), columns=['Common words', f'{hazard}'])

  fr_df = fr_df.set_index('Common words')
  fr_df = fr_df.reindex(index=all_df['All words'])
  fr_df = fr_df.reset_index()
  fr_df = fr_df.fillna(0)

  return fr_df

frequency_df = pd.concat([all_df['All words'], freq_df('biological').iloc[:,1],freq_df('chemical').iloc[:,1],
           freq_df('fraud').iloc[:,1],freq_df('food contact materials').iloc[:,1],freq_df('allergens').iloc[:,1],
           freq_df('food additives and \n flavourings').iloc[:,1],freq_df('foreign bodies').iloc[:,1],freq_df('organoleptic aspects').iloc[:,1],
           freq_df('other hazard').iloc[:,1]], axis=1, keys=['All words', 'biological','chemical','fraud','food contact materials','allergens','food additives and \n flavourings',
                                                             'foreign bodies','organoleptic aspects','other hazard'])

#the total number of tokens
total_tokens = sum(all_df['Frequency'])
frequency_df.set_index('All words', inplace=True)

#freq=(num of occurrence)/(total tokens)
frequency_df = (frequency_df/total_tokens)*100

#Generate a custom diverging colormap
fig, ax = plt.subplots(figsize=(8,8))
cmap = sns.diverging_palette(240, 10, sep=100, as_cmap=True)

#Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(frequency_df.corr(), cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={"shrink": .5})